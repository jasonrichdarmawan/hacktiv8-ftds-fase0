{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzNU0dHMz_96"
      },
      "source": [
        "# Spark Example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQv3sFs20Ass",
        "outputId": "d34d102e-f576-4754-a011-6b825574aa79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "Nm5dRFaW2ZaE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "JG14xulV2oaz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3dn-XHs38hS",
        "outputId": "4a44f615-a9af-458e-b4ec-04b3c263e6b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.3.1-bin-hadoop3  spark-3.3.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ['SPARK_HOME'] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable"
      ],
      "metadata": {
        "id": "liRKRutc3lqi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.executable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RD5JL_U44E4m",
        "outputId": "f7060a83-7681-44ba-cd81-05047fcccafe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/bin/python3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "gajDej8g4HAf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg7fxESez_98"
      },
      "source": [
        "PySpark applications start with initializing SparkSession which is the entry point of PySpark as below. In case of running it in PySpark shell via pyspark executable, the shell automatically creates the session in the variable spark for users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iqILCllFz_98"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2VXf3Mfz_99"
      },
      "source": [
        "## DataFrame Creation\n",
        "\n",
        "A PySpark DataFrame can be created via pyspark.sql.SparkSession.createDataFrame typically by passing a list of lists, tuples, dictionaries and pyspark.sql.Rows, a pandas DataFrame and an RDD consisting of such a list. pyspark.sql.SparkSession.createDataFrame takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data.\n",
        "\n",
        "Firstly, you can create a PySpark DataFrame from a list of rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ2CQQd6z_99",
        "outputId": "2e733b49-7dfe-40c7-a5b2-f49f47ed239d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
        "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
        "])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HntgnzPPz_9-"
      },
      "source": [
        "Create a PySpark DataFrame with an explicit schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wy6KFPaz_9-",
        "outputId": "0b1616b6-15c0-44ea-d20c-4f25cfc47e46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df = spark.createDataFrame([\n",
        "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
        "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
        "], schema='a long, b double, c string, d date, e timestamp')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsRx516Oz_9-"
      },
      "source": [
        "Create a PySpark DataFrame from a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lknI8c_z_9_",
        "outputId": "d9fee000-7321-40c3-9e0c-4acdc15cfbde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "pandas_df = pd.DataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
        "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
        "})\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKmn3Tk_z_9_"
      },
      "source": [
        "Create a PySpark DataFrame from an RDD consisting of a list of tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrGZFN7_z_9_",
        "outputId": "e92ee6b0-1bd7-4718-c65b-7cf65958b3de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "rdd = spark.sparkContext.parallelize([\n",
        "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
        "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
        "])\n",
        "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCLoHkdz_-A"
      },
      "source": [
        "The DataFrames created above all have the same results and schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yuHvduFz_-A",
        "outputId": "9ec56da3-7462-48ab-a9b0-0272bab2469e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n",
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            " |-- d: date (nullable = true)\n",
            " |-- e: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# All DataFrames above result same.\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSI4KBEoz_-A"
      },
      "source": [
        "## Viewing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PXhxMwLz_-A"
      },
      "source": [
        "The top rows of a DataFrame can be displayed using DataFrame.show()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlI2GTriz_-A",
        "outputId": "a033dce8-e316-493e-8bf1-c25a54af3c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U38WGgZ9z_-A"
      },
      "source": [
        "Alternatively, you can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via spark.sql.repl.eagerEval.maxNumRows configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq4OOfbAz_-B",
        "outputId": "df3fccaa-7fbd-4303-d5e5-4e2193dd7629"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
              "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
              "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
              "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng7Vp12bz_-B"
      },
      "source": [
        "The rows can also be shown vertically. This is useful when rows are too long to show horizontally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xISoQ_a1z_-B",
        "outputId": "5eb5e2f1-1b83-4787-95c9-6ac868b8bbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-RECORD 0------------------\n",
            " a   | 1                   \n",
            " b   | 2.0                 \n",
            " c   | string1             \n",
            " d   | 2000-01-01          \n",
            " e   | 2000-01-01 12:00:00 \n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(1, vertical=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcBI6XW2z_-B"
      },
      "source": [
        "You can see the DataFrame’s schema and column names as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51m9YJ6Qz_-B",
        "outputId": "602d2c26-0de8-421f-a582-9c04f21d4973"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'b', 'c', 'd', 'e']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUjZWYOTz_-B",
        "outputId": "b0211e84-4d47-422d-c432-eb043c221735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- a: long (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- c: string (nullable = true)\n",
            " |-- d: date (nullable = true)\n",
            " |-- e: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nHgzDtz_-B"
      },
      "source": [
        "Show the summary of the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ae4fnKbxz_-C",
        "outputId": "e541473a-1959-498e-92b1-449d26024aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+---+-------+\n",
            "|summary|  a|  b|      c|\n",
            "+-------+---+---+-------+\n",
            "|  count|  3|  3|      3|\n",
            "|   mean|2.0|3.0|   null|\n",
            "| stddev|1.0|1.0|   null|\n",
            "|    min|  1|2.0|string1|\n",
            "|    max|  3|4.0|string3|\n",
            "+-------+---+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"a\", \"b\", \"c\").describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PQjaKCxz_-C"
      },
      "source": [
        "DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3NriSBjHz_-C",
        "outputId": "44774432-85c8-40f5-d911-ed10383e9972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
              " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
              " Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWql5XFJz_-C"
      },
      "source": [
        "In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "83ATBpnCz_-C",
        "outputId": "8a8734fa-a800-419e-cf00-5323d3d7f411"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOENG68mz_-C"
      },
      "source": [
        "PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas APIs. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dGLFWa1pz_-C",
        "outputId": "60d1d405-09ce-4782-bc74-2baee1c96e03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>string1</td>\n",
              "      <td>2000-01-01</td>\n",
              "      <td>2000-01-01 12:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>string2</td>\n",
              "      <td>2000-02-01</td>\n",
              "      <td>2000-01-02 12:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>string3</td>\n",
              "      <td>2000-03-01</td>\n",
              "      <td>2000-01-03 12:00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a    b        c           d                   e\n",
              "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
              "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
              "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMxppWIwz_-C"
      },
      "source": [
        "## Selecting and Accessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jk2ZrgYz_-C"
      },
      "source": [
        "PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoATCVlaz_-C",
        "outputId": "1ff5a276-4e73-4c6a-9b94-5739a4ad6fcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Column<'a'>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbKYcNiwz_-C",
        "outputId": "42308e0b-3d8a-46c2-e1d6-71d28cde8ca7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import Column\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZZgEFvez_-C"
      },
      "source": [
        "These Columns can be used to select the columns from a DataFrame. For example, DataFrame.select() takes the Column instances that returns another DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTddZHd2z_-C",
        "outputId": "7fa148ee-2d25-440b-ae8d-1db62b2f6906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|      c|\n",
            "+-------+\n",
            "|string1|\n",
            "|string2|\n",
            "|string3|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df.c).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7fe6Dnyz_-C"
      },
      "source": [
        "Assign new Column instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4JzUfSAYz_-C",
        "outputId": "89f79e49-435c-46a1-81a2-490dda3e7295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+-------+----------+-------------------+-------+\n",
            "|  a|  b|      c|         d|                  e|upper_c|\n",
            "+---+---+-------+----------+-------------------+-------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
            "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
            "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
            "+---+---+-------+----------+-------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn('upper_c', upper(df.c)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47BzqTALz_-C"
      },
      "source": [
        "To select a subset of rows, use DataFrame.filter()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NOkpON8vz_-D",
        "outputId": "749fa710-1032-493a-ba3b-407927fbf50e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.a == 1).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmgxbx3mz_-D"
      },
      "source": [
        "## Applying a Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaIsrEvz_-D"
      },
      "source": [
        "PySpark supports various UDFs and APIs to allow users to execute Python native functions. See also the latest Pandas UDFs and Pandas Function APIs. For instance, the example below allows users to directly use the APIs in a pandas Series within Python native function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuFQ8WP-z_-D"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('long')\n",
        "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
        "    # Simply plus one by using pandas Series.\n",
        "    return series + 1\n",
        "\n",
        "df.select(pandas_plus_one(df.a)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8jxtFzz_-D"
      },
      "source": [
        "Another example is DataFrame.mapInPandas which allows users directly use the APIs in a pandas DataFrame without any restrictions such as the result length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpjoL6o0z_-D",
        "outputId": "5c04c1cd-7acd-450a-be10-c4f334c7c39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def pandas_filter_func(iterator):\n",
        "    for pandas_df in iterator:\n",
        "        yield pandas_df[pandas_df.a == 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WGplUO7z_-D"
      },
      "source": [
        "## Grouping Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmtz1ecOz_-D"
      },
      "source": [
        "PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "argqwb3_z_-D",
        "outputId": "0a722db9-811c-4dae-cbe1-0cd186b5a017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+---+---+\n",
            "|color| fruit| v1| v2|\n",
            "+-----+------+---+---+\n",
            "|  red|banana|  1| 10|\n",
            "| blue|banana|  2| 20|\n",
            "|  red|carrot|  3| 30|\n",
            "| blue| grape|  4| 40|\n",
            "|  red|carrot|  5| 50|\n",
            "|black|carrot|  6| 60|\n",
            "|  red|banana|  7| 70|\n",
            "|  red| grape|  8| 80|\n",
            "+-----+------+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
        "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4rIblPAz_-D"
      },
      "source": [
        "Grouping and then applying the avg() function to the resulting groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vHFLoLDWz_-D",
        "outputId": "89e62589-e92a-483b-ad53-64a98040c0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-------+-------+\n",
            "|color|avg(v1)|avg(v2)|\n",
            "+-----+-------+-------+\n",
            "|  red|    4.8|   48.0|\n",
            "|black|    6.0|   60.0|\n",
            "| blue|    3.0|   30.0|\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupby('color').avg().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mwHMQM4z_-D"
      },
      "source": [
        "You can also apply a Python native function against each group by using pandas APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaDsFP9_z_-D",
        "outputId": "4f7b5f7f-1cd3-444f-98d6-6fa5eb9f736a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+---+---+\n",
            "|color| fruit| v1| v2|\n",
            "+-----+------+---+---+\n",
            "|  red|banana| -3| 10|\n",
            "|  red|carrot| -1| 30|\n",
            "|  red|carrot|  0| 50|\n",
            "|  red|banana|  2| 70|\n",
            "|  red| grape|  3| 80|\n",
            "|black|carrot|  0| 60|\n",
            "| blue|banana| -1| 20|\n",
            "| blue| grape|  1| 40|\n",
            "+-----+------+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def plus_mean(pandas_df):\n",
        "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
        "\n",
        "df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1gOyQboz_-D"
      },
      "source": [
        "Co-grouping and applying a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOJ9W0WGz_-D",
        "outputId": "a6dd44c9-93d9-4367-c746-cc7afac5827d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+---+---+\n",
            "|    time| id| v1| v2|\n",
            "+--------+---+---+---+\n",
            "|20000101|  1|1.0|  x|\n",
            "|20000102|  1|3.0|  x|\n",
            "|20000101|  2|2.0|  y|\n",
            "|20000102|  2|4.0|  y|\n",
            "+--------+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
        "    ('time', 'id', 'v1'))\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
        "    ('time', 'id', 'v2'))\n",
        "\n",
        "def asof_join(l, r):\n",
        "    return pd.merge_asof(l, r, on='time', by='id')\n",
        "\n",
        "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n",
        "    asof_join, schema='time int, id int, v1 double, v2 string').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yue_KDS7z_-D"
      },
      "source": [
        "## Getting Data in/out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jf-Z8dnz_-D"
      },
      "source": [
        "CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster.\n",
        "\n",
        "There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc. See also the latest Spark SQL, DataFrames and Datasets Guide in Apache Spark documentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JktZ6Ltjz_-D",
        "outputId": "ce085b28-a1b7-4cb3-92a0-eb1194872c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+---+---+\n",
            "|color| fruit| v1| v2|\n",
            "+-----+------+---+---+\n",
            "|black|carrot|  6| 60|\n",
            "| blue|banana|  2| 20|\n",
            "|  red|banana|  1| 10|\n",
            "|  red|carrot|  3| 30|\n",
            "| blue| grape|  4| 40|\n",
            "|  red|carrot|  5| 50|\n",
            "|  red|banana|  7| 70|\n",
            "|  red| grape|  8| 80|\n",
            "+-----+------+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.write.csv('ftds.csv', header=True)\n",
        "spark.read.csv('ftds.csv', header=True).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ki448Wlz_-E"
      },
      "source": [
        "## Working with SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qYpTfB5z_-E"
      },
      "source": [
        "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sniMhjYz_-E",
        "outputId": "d6781e6b-0c2c-48e9-b8ac-c0fb6048f0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       8|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView(\"tableA\")\n",
        "spark.sql(\"SELECT count(*) from tableA\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHadwuW6z_-E"
      },
      "source": [
        "In addition, UDFs can be registered and invoked in SQL out of the box:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdRDvKR3z_-E",
        "outputId": "22af8b40-b2e3-431a-8879-b8602d2f815d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|add_one(v1)|\n",
            "+-----------+\n",
            "|          2|\n",
            "|          3|\n",
            "|          4|\n",
            "|          5|\n",
            "|          6|\n",
            "|          7|\n",
            "|          8|\n",
            "|          9|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(\"integer\")\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "    return s + 1\n",
        "\n",
        "spark.udf.register(\"add_one\", add_one)\n",
        "spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WAbcs9Kz_-E"
      },
      "source": [
        "These SQL expressions can directly be mixed and used as PySpark columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5XfwVGmz_-E",
        "outputId": "e5c16905-228c-4b38-f9c1-5f1b5882fefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|add_one(v1)|\n",
            "+-----------+\n",
            "|          2|\n",
            "|          3|\n",
            "|          4|\n",
            "|          5|\n",
            "|          6|\n",
            "|          7|\n",
            "|          8|\n",
            "|          9|\n",
            "+-----------+\n",
            "\n",
            "+--------------+\n",
            "|(count(1) > 0)|\n",
            "+--------------+\n",
            "|          true|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "df.selectExpr('add_one(v1)').show()\n",
        "df.select(expr('count(*)') > 0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shudzirnz_-E"
      },
      "source": [
        "## Spark Data Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS1UFWwSz_-E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import date, timedelta, datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUSanexIz_-E"
      },
      "source": [
        "First of all, a Spark session needs to be initialized. With the help of SparkSession, DataFrame can be created and registered as tables. Moreover, SQL tables are executed, tables can be cached, and parquet/JSON/CSV/Avro data formatted files can be read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMXJ7j72z_-E"
      },
      "outputs": [],
      "source": [
        "sc = SparkSession.builder.appName(\"PysparkFTDS\")\\\n",
        ".config (\"spark.sql.shuffle.partitions\", \"50\")\\\n",
        ".config(\"spark.driver.maxResultSize\",\"5g\")\\\n",
        ".config (\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8N8nDUxz_-E"
      },
      "source": [
        "### Creating DataFrame\n",
        "\n",
        "dataset [here](https://www.kaggle.com/cmenca/new-york-times-hardcover-fiction-best-sellers)\n",
        "\n",
        "DataFrames can be created by reading text, CSV, JSON, and Parquet file formats. In our example, we will be using a .json formatted file. You can also find and read text, CSV, and Parquet file formats by using the related read functions as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-6rBr8Yz_-E"
      },
      "outputs": [],
      "source": [
        "#Creates a spark data frame called as raw_data.\n",
        "\n",
        "#JSON\n",
        "dataframe = sc.read.json('nyt2.json')\n",
        "\n",
        "#TXT FILES\n",
        "# dataframe_txt = sc.read.text('text_data.txt')\n",
        "\n",
        "#CSV FILES\n",
        "# dataframe_csv = sc.read.csv('csv_data.csv')\n",
        "\n",
        "#PARQUET FILES\n",
        "# dataframe_parquet = sc.read.load('parquet_data.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkaB1Qu8z_-E"
      },
      "source": [
        "### Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fVwCegzZz_-E",
        "outputId": "9d05bc7b-9e98-442c-aafb-7314d56ba59e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
            "|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|\n",
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|       Putnam| {4}|           {0}|           THE FRONT|          {1}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|    Doubleday| {5}|           {0}|               SNUFF|          {1}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, null}|{{1212883200000}}|Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANY’S|          {4}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, null}|{{1212883200000}}|       Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, null}|{{1212883200000}}|Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, null}|{{1212883200000}}|       Harper| {9}|           {8}|     CARELESS IN RED|          {3}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, null}|{{1212883200000}}|Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|\n",
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataframe.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToYDf4Tlz_-E"
      },
      "source": [
        "Duplicate values in a table can be eliminated by using dropDuplicates() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr3UaUyYz_-E",
        "outputId": "aa76ba8e-7a78-4dd5-d4cd-d4579b085b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
            "|                 _id|  amazon_product_url|              author| bestsellers_date|         description|        price|   published_date|       publisher|rank|rank_last_week|               title|weeks_on_list|\n",
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Clive Cussler wit...|{{1213401600000}}|Juan Cabrillo and...|{26.95, null}|{{1214697600000}}|          Putnam| {4}|           {3}|         PLAGUE SHIP|          {2}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|      Jeffery Deaver|{{1215820800000}}|Detectives Lincol...|    {null, 0}|{{1217116800000}}|Simon & Schuster|{20}|           {0}|   THE BROKEN WINDOW|          {0}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|        Daniel Silva|{{1217030400000}}|Gabriel Allon, an...|{26.95, null}|{{1218326400000}}|          Putnam| {1}|           {0}|  THE SECRET SERVANT|          {1}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Janet Evanovich|{{1217030400000}}|Stephanie Plum an...|{27.95, null}|{{1218326400000}}|    St. Martin’s| {9}|           {7}|   FEARLESS FOURTEEN|          {6}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|          Jane Green|{{1218240000000}}|A woman’s life ch...|    {null, 0}|{{1219536000000}}|          Viking|{18}|           {0}|     THE BEACH HOUSE|          {0}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|      Brunonia Barry|{{1220054400000}}|Secrets of a fami...|{24.95, null}|{{1221350400000}}|          Morrow|{13}|          {10}|     THE LACE READER|          {5}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    David Wroblewski|{{1221264000000}}|A mute takes refu...|{25.95, null}|{{1222560000000}}|            Ecco| {9}|           {9}|THE STORY OF EDGA...|         {14}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1225497600000}}|Virgil Flowers in...|    {null, 0}|{{1226793600000}}|          Putnam|{20}|           {0}|      HEAT LIGHTNING|          {0}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|            J D Robb|{{1226102400000}}|Lt. Eve Dallas in...|{25.95, null}|{{1227398400000}}|          Putnam| {2}|           {0}|  SALVATION IN DEATH|          {1}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       Toni Morrison|{{1226707200000}}|In 17th-century A...|{23.95, null}|{{1228003200000}}|           Knopf| {5}|           {0}|             A MERCY|          {1}|\n",
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+----------------+----+--------------+--------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataframe_dropdup = dataframe.dropDuplicates()\n",
        "dataframe_dropdup.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezHT1eLRz_-E"
      },
      "source": [
        "### Queries\n",
        "\n",
        "\"Select\" Operation\n",
        "\n",
        "It is possible to obtain columns by attribute `(\"author\")` or by indexing `(dataframe['author'])`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74gMwowz_-E",
        "outputId": "8f6192ee-31b9-4649-9cba-1f3baab92921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|              author|\n",
            "+--------------------+\n",
            "|       Dean R Koontz|\n",
            "|     Stephenie Meyer|\n",
            "|        Emily Giffin|\n",
            "|   Patricia Cornwell|\n",
            "|     Chuck Palahniuk|\n",
            "|James Patterson a...|\n",
            "|       John Sandford|\n",
            "|       Jimmy Buffett|\n",
            "|    Elizabeth George|\n",
            "|      David Baldacci|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------------------+--------------------+----+-------------+\n",
            "|              author|               title|rank|        price|\n",
            "+--------------------+--------------------+----+-------------+\n",
            "|       Dean R Koontz|           ODD HOURS| {1}|   {null, 27}|\n",
            "|     Stephenie Meyer|            THE HOST| {2}|{25.99, null}|\n",
            "|        Emily Giffin|LOVE THE ONE YOU'...| {3}|{24.95, null}|\n",
            "|   Patricia Cornwell|           THE FRONT| {4}|{22.95, null}|\n",
            "|     Chuck Palahniuk|               SNUFF| {5}|{24.95, null}|\n",
            "|James Patterson a...|SUNDAYS AT TIFFANY’S| {6}|{24.99, null}|\n",
            "|       John Sandford|        PHANTOM PREY| {7}|{26.95, null}|\n",
            "|       Jimmy Buffett|          SWINE NOT?| {8}|{21.99, null}|\n",
            "|    Elizabeth George|     CARELESS IN RED| {9}|{27.95, null}|\n",
            "|      David Baldacci|     THE WHOLE TRUTH|{10}|{26.99, null}|\n",
            "+--------------------+--------------------+----+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Show all entries in title column\n",
        "dataframe.select(\"author\").show(10)\n",
        "\n",
        "#Show all entries in title, author, rank, price columns\n",
        "dataframe.select(\"author\", \"title\", \"rank\", \"price\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o2DN_Wtz_-E"
      },
      "source": [
        "The 'title' column is selected and a condition is added with a 'when' condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEWc_FDUz_-F",
        "outputId": "36a53d10-b32d-4e70-a7a5-8503b91f3220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------------------------------------------------+\n",
            "|               title|CASE WHEN (NOT (title = ODD HOURS)) THEN 1 ELSE 0 END|\n",
            "+--------------------+-----------------------------------------------------+\n",
            "|           ODD HOURS|                                                    0|\n",
            "|            THE HOST|                                                    1|\n",
            "|LOVE THE ONE YOU'...|                                                    1|\n",
            "|           THE FRONT|                                                    1|\n",
            "|               SNUFF|                                                    1|\n",
            "|SUNDAYS AT TIFFANY’S|                                                    1|\n",
            "|        PHANTOM PREY|                                                    1|\n",
            "|          SWINE NOT?|                                                    1|\n",
            "|     CARELESS IN RED|                                                    1|\n",
            "|     THE WHOLE TRUTH|                                                    1|\n",
            "+--------------------+-----------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show title and assign 0 or 1 depending on title\n",
        "dataframe.select(\"title\", when(dataframe.title != 'ODD HOURS', 1).otherwise(0)).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODXfq_ygz_-F"
      },
      "source": [
        "The “isin” operation is applied instead of “when” which can be also used to define some conditions to rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seoWZcESz_-F",
        "outputId": "5e83b487-da21-4525-ad6b-4da0c3da42a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-------------+-----------------+--------------------+-------------+-----------------+------------+----+--------------+--------------------+-------------+\n",
            "|                 _id|  amazon_product_url|       author| bestsellers_date|         description|        price|   published_date|   publisher|rank|rank_last_week|               title|weeks_on_list|\n",
            "+--------------------+--------------------+-------------+-----------------+--------------------+-------------+-----------------+------------+----+--------------+--------------------+-------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...| Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}|St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, null}|{{1212883200000}}|      Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...| Emily Giffin|{{1212192000000}}|A woman’s happy m...|{24.95, null}|{{1213488000000}}|St. Martin’s| {4}|           {3}|LOVE THE ONE YOU'...|          {3}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|John Sandford|{{1212192000000}}|The Minneapolis d...|{26.95, null}|{{1213488000000}}|      Putnam| {9}|           {7}|        PHANTOM PREY|          {4}|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...| Emily Giffin|{{1212796800000}}|A woman’s happy m...|{24.95, null}|{{1214092800000}}|St. Martin’s| {4}|           {4}|LOVE THE ONE YOU'...|          {4}|\n",
            "+--------------------+--------------------+-------------+-----------------+--------------------+-------------+-----------------+------------+----+--------------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show rows with specified authors if in the given options\n",
        "\n",
        "dataframe [dataframe.author.isin(\"John Sandford\", \"Emily Giffin\")].show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T-AuBblz_-F"
      },
      "source": [
        "In the brackets of the “Like” function, the % character is used to filter out all titles having the “ THE ” word. If the condition we are looking for is the exact match, then no % character shall be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "XXE6v0eBz_-F",
        "outputId": "0c87a6c7-76af-47a4-f81c-3cd50d30f780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------------+\n",
            "|              author|               title|title LIKE % THE %|\n",
            "+--------------------+--------------------+------------------+\n",
            "|       Dean R Koontz|           ODD HOURS|             false|\n",
            "|     Stephenie Meyer|            THE HOST|             false|\n",
            "|        Emily Giffin|LOVE THE ONE YOU'...|              true|\n",
            "|   Patricia Cornwell|           THE FRONT|             false|\n",
            "|     Chuck Palahniuk|               SNUFF|             false|\n",
            "|James Patterson a...|SUNDAYS AT TIFFANY’S|             false|\n",
            "|       John Sandford|        PHANTOM PREY|             false|\n",
            "|       Jimmy Buffett|          SWINE NOT?|             false|\n",
            "|    Elizabeth George|     CARELESS IN RED|             false|\n",
            "|      David Baldacci|     THE WHOLE TRUTH|             false|\n",
            "|        Troy Denning|          INVINCIBLE|             false|\n",
            "|          James Frey|BRIGHT SHINY MORNING|             false|\n",
            "|         Garth Stein|THE ART OF RACING...|              true|\n",
            "|     Debbie Macomber|       TWENTY WISHES|             false|\n",
            "|         Jeff Shaara|      THE STEEL WAVE|             false|\n",
            "+--------------------+--------------------+------------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show author and title is TRUE if title has \" THE \" word in titles\n",
        "\n",
        "dataframe.select(\"author\", \"title\", dataframe.title.like(\"% THE %\")).show(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWHZo3zJz_-F"
      },
      "source": [
        "StartsWith scans from the beginning of word/content with specified criteria in the brackets. In parallel, EndsWith processes the word/content starting from the end. Both of the functions are case-sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OImPUNnHz_-F",
        "outputId": "3029a6ff-47e0-4084-b893-fcfc21b4ac64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+--------------------+----------------------+\n",
            "|           author|               title|startswith(title, THE)|\n",
            "+-----------------+--------------------+----------------------+\n",
            "|    Dean R Koontz|           ODD HOURS|                 false|\n",
            "|  Stephenie Meyer|            THE HOST|                  true|\n",
            "|     Emily Giffin|LOVE THE ONE YOU'...|                 false|\n",
            "|Patricia Cornwell|           THE FRONT|                  true|\n",
            "|  Chuck Palahniuk|               SNUFF|                 false|\n",
            "+-----------------+--------------------+----------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+--------------------+-------------------+\n",
            "|           author|               title|endswith(title, NT)|\n",
            "+-----------------+--------------------+-------------------+\n",
            "|    Dean R Koontz|           ODD HOURS|              false|\n",
            "|  Stephenie Meyer|            THE HOST|              false|\n",
            "|     Emily Giffin|LOVE THE ONE YOU'...|              false|\n",
            "|Patricia Cornwell|           THE FRONT|               true|\n",
            "|  Chuck Palahniuk|               SNUFF|              false|\n",
            "+-----------------+--------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataframe.select(\"author\", \"title\", dataframe.title.startswith(\"THE\")).show(5)\n",
        "\n",
        "dataframe.select(\"author\", \"title\", dataframe.title.endswith(\"NT\")).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ssg25caz_-F"
      },
      "source": [
        "Substring functions to extract the text between specified indexes. In the following examples, texts are extracted from the index numbers (1, 3), (3, 6), and (1, 6)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwH-FtRHz_-F",
        "outputId": "1f45dfe6-247c-4727-c1b0-9efe407e0139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|title|\n",
            "+-----+\n",
            "|  Dea|\n",
            "|  Ste|\n",
            "|  Emi|\n",
            "|  Pat|\n",
            "|  Chu|\n",
            "+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+\n",
            "| title|\n",
            "+------+\n",
            "|an R K|\n",
            "|epheni|\n",
            "|ily Gi|\n",
            "|tricia|\n",
            "|uck Pa|\n",
            "+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+\n",
            "| title|\n",
            "+------+\n",
            "|Dean R|\n",
            "|Stephe|\n",
            "|Emily |\n",
            "|Patric|\n",
            "|Chuck |\n",
            "+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataframe.select(dataframe.author.substr(1, 3).alias(\"title\")).show(5)\n",
        "dataframe.select(dataframe.author.substr(3, 6).alias(\"title\")).show(5)\n",
        "dataframe.select(dataframe.author.substr(1, 6).alias(\"title\")).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHCgdSYyz_-F"
      },
      "source": [
        "Adding Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_qJKiNYz_-F",
        "outputId": "5190d917-72b7-4d29-b138-1509ffedb2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|  amazon_product_url|           author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|       Putnam| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|    Doubleday| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lit() is required while we are creating columns with exact values.\n",
        "\n",
        "dataframe = dataframe.withColumn('new_column', lit('This is a new column'))\n",
        "\n",
        "dataframe.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "broXXZzCz_-F"
      },
      "source": [
        "Updating Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfSh4p-fz_-F",
        "outputId": "8343f1a3-f31f-41c3-d194-b521578352cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|           author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|       Putnam| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|    Doubleday| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataframe = dataframe.withColumnRenamed('amazon_product_url', 'URL')\n",
        "\n",
        "dataframe.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3oTTgh3z_-F"
      },
      "source": [
        "Removing Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUovVhyNz_-F",
        "outputId": "8182c353-7db6-4b63-b344-ac60994f8c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|           author| bestsellers_date|         description|        price|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|           author| bestsellers_date|         description|        price|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataframe_remove = dataframe.drop(\"publisher\", \"published_date\").show(5)\n",
        "\n",
        "dataframe_remove2 = dataframe.drop(dataframe.publisher).drop(dataframe.published_date).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a0r-yLwz_-F"
      },
      "source": [
        "### Inspect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URYHWZWwz_-F",
        "outputId": "a1c967e0-f786-40f9-d91b-a67e4fcfef80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|              author| bestsellers_date|         description|        price|   published_date|           publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|              Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|       Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|        Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}|        St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|   Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|              Putnam| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|           Doubleday| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|James Patterson a...|{{1211587200000}}|A woman finds an ...|{24.99, null}|{{1212883200000}}|       Little, Brown| {6}|           {3}|SUNDAYS AT TIFFANY’S|          {4}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       John Sandford|{{1211587200000}}|The Minneapolis d...|{26.95, null}|{{1212883200000}}|              Putnam| {7}|           {4}|        PHANTOM PREY|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       Jimmy Buffett|{{1211587200000}}|A Southern family...|{21.99, null}|{{1212883200000}}|       Little, Brown| {8}|           {6}|          SWINE NOT?|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Elizabeth George|{{1211587200000}}|In Cornwall, tryi...|{27.95, null}|{{1212883200000}}|              Harper| {9}|           {8}|     CARELESS IN RED|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|      David Baldacci|{{1211587200000}}|An intelligence a...|{26.99, null}|{{1212883200000}}|       Grand Central|{10}|           {7}|     THE WHOLE TRUTH|          {5}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|        Troy Denning|{{1211587200000}}|The New Jedi orde...|   {null, 27}|{{1212883200000}}|  Del Rey/Ballantine|{11}|           {5}|          INVINCIBLE|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|          James Frey|{{1211587200000}}|A novel, set in L...|{26.95, null}|{{1212883200000}}|              Harper|{12}|           {9}|BRIGHT SHINY MORNING|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|         Garth Stein|{{1211587200000}}|A Lab-terrier mix...|{23.95, null}|{{1212883200000}}|              Harper|{13}|           {0}|THE ART OF RACING...|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Debbie Macomber|{{1211587200000}}|A widow who owns ...|{24.95, null}|{{1212883200000}}|                Mira|{14}|          {10}|       TWENTY WISHES|          {4}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|         Jeff Shaara|{{1211587200000}}|A novel about the...|   {null, 28}|{{1212883200000}}|          Ballantine|{15}|          {11}|      THE STEEL WAVE|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Phillip Margolin|{{1211587200000}}|                    |    {null, 0}|{{1212883200000}}|HarperCollins Pub...|{16}|           {0}| EXECUTIVE PRIVILEGE|          {0}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       Jhumpa Lahiri|{{1211587200000}}|Stories of the an...|    {null, 0}|{{1212883200000}}|               Knopf|{17}|           {0}|  UNACCUSTOMED EARTH|          {0}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|      Joseph O'Neill|{{1211587200000}}|A Dutchman desert...|    {null, 0}|{{1212883200000}}|Knopf Publishing ...|{18}|           {0}|          NETHERLAND|          {0}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|        John Grisham|{{1211587200000}}|Political and leg...|    {null, 0}|{{1212883200000}}|Doubleday Publishing|{19}|           {0}|          THE APPEAL|          {0}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|       James Rollins|{{1211587200000}}|                    |    {null, 0}|{{1212883200000}}|Random House Publ...|{20}|           {0}|INDIANA JONES AND...|          {0}|This is a new column|\n",
            "+--------------------+--------------------+--------------------+-----------------+--------------------+-------------+-----------------+--------------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------+--------------------+---------------+--------------------+---------+------------------+--------------------+\n",
            "|summary|                 URL|         author|         description|publisher|             title|          new_column|\n",
            "+-------+--------------------+---------------+--------------------+---------+------------------+--------------------+\n",
            "|  count|               10195|          10195|               10195|    10195|             10195|               10195|\n",
            "|   mean|                null|           null|                null|     null|1877.7142857142858|                null|\n",
            "| stddev|                null|           null|                null|     null| 370.9760613506458|                null|\n",
            "|    min|http://www.amazon...|        AJ Finn|                    |      ACE|  10TH ANNIVERSARY|This is a new column|\n",
            "|    max|https://www.amazo...|various authors|’Tis for the Rebe...|allantine|               ZOO|This is a new column|\n",
            "+-------+--------------------+---------------+--------------------+---------+------------------+--------------------+\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [_id#627, amazon_product_url#628 AS URL#1039, author#629, bestsellers_date#630, description#631, price#632, published_date#633, publisher#634, rank#635, rank_last_week#636, title#637, weeks_on_list#638, This is a new column AS new_column#960]\n",
            "+- FileScan json [_id#627,amazon_product_url#628,author#629,bestsellers_date#630,description#631,price#632,published_date#633,publisher#634,rank#635,rank_last_week#636,title#637,weeks_on_list#638] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/D:/spark/nyt2.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_id:struct<$oid:string>,amazon_product_url:string,author:string,bestsellers_date:struct<$d...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Returns dataframe column names and data types\n",
        "dataframe.dtypes\n",
        "\n",
        "# Displays the content of dataframe\n",
        "dataframe.show()\n",
        "\n",
        "# Return first n rows\n",
        "dataframe.head()\n",
        "\n",
        "# Returns first row\n",
        "dataframe.first()\n",
        "\n",
        "# Return first n rows\n",
        "dataframe.take(5)\n",
        "\n",
        "# Computes summary statistics\n",
        "dataframe.describe().show()\n",
        "\n",
        "# Returns columns of dataframe\n",
        "dataframe.columns\n",
        "\n",
        "# Counts the number of rows in dataframe\n",
        "dataframe.count()\n",
        "\n",
        "# Counts the number of distinct rows in dataframe\n",
        "dataframe.distinct().count()\n",
        "\n",
        "# Prints plans including physical and logical\n",
        "dataframe.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "356kR0Taz_-F"
      },
      "source": [
        "### GroupBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VRN36Mw7z_-F",
        "outputId": "55a0e625-a6c3-4274-c170-43231ee018f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|              author|count|\n",
            "+--------------------+-----+\n",
            "|          James Frey|    2|\n",
            "|    Elin Hilderbrand|   58|\n",
            "|   Sharon Kay Penman|    2|\n",
            "|         Kate Jacobs|    3|\n",
            "|       Karen Robards|    6|\n",
            "|     Gary Shteyngart|    3|\n",
            "|         Lisa Genova|    7|\n",
            "|James Patterson a...|   30|\n",
            "|         Ruth Reichl|    3|\n",
            "|         JRR Tolkien|    2|\n",
            "+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Group by author, count the books of the authors in the groups\n",
        "\n",
        "dataframe.groupBy(\"author\").count().show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAcYdY8Jz_-F"
      },
      "source": [
        "### Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2amKbU0z_-G",
        "outputId": "4b66da92-fec9-4445-87cb-98340176c4af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
            "|                 _id|                 URL|         author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|   title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1212192000000}}|Aliens have taken...|{25.99, null}|{{1213488000000}}|Little, Brown| {2}|           {2}|THE HOST|          {4}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1212796800000}}|Aliens have taken...|{25.99, null}|{{1214092800000}}|Little, Brown| {2}|           {2}|THE HOST|          {5}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1213401600000}}|Aliens have taken...|{25.99, null}|{{1214697600000}}|Little, Brown| {3}|           {2}|THE HOST|          {6}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1214006400000}}|Aliens have taken...|{25.99, null}|{{1215302400000}}|Little, Brown| {3}|           {3}|THE HOST|          {7}|This is a new column|\n",
            "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filtering entries of title\n",
        "# Only keeps records having value 'THE HOST'\n",
        "\n",
        "dataframe.filter(dataframe[\"title\"] == 'THE HOST').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1o5Leciz_-G"
      },
      "source": [
        "### Missing & Replacing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW-joHelz_-G",
        "outputId": "1e97b223-1afc-4308-ff2a-548da575cd2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|           author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|       Putnam| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|    Doubleday| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|           author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|       Putnam| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|    Doubleday| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|           author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|{{1211587200000}}|A Massachusetts s...|{22.95, null}|{{1212883200000}}|       Putnam| {4}|           {0}|           THE FRONT|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|{{1211587200000}}|An aging porn que...|{24.95, null}|{{1212883200000}}|    Doubleday| {5}|           {0}|               SNUFF|          {1}|This is a new column|\n",
            "+--------------------+--------------------+-----------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Replace null values\n",
        "dataframe.na.fill(50).show(5)\n",
        "\n",
        "# Return new dataframe restricting rows with null values\n",
        "dataframe.na.drop().show(5)\n",
        "\n",
        "# Return new dataframe replacing one value with another\n",
        "dataframe.na.replace(10, 20).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEqOskP4z_-G"
      },
      "source": [
        "### Repartitioning\n",
        "\n",
        "It is possible to increase or decrease the existing level of partitioning in RDD Increasing can be actualized by using the repartition(self, numPartitions) function which results in a new RDD that obtains the higher number of partitions. Decreasing can be processed with coalesce(self, numPartitions, shuffle=False) function that results in a new RDD with a reduced number of partitions to a specified number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDr32nJGz_-G",
        "outputId": "dd84d57a-7197-4191-8cf1-c54117269560"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dataframe with 10 partitions\n",
        "dataframe.repartition(10).rdd.getNumPartitions()\n",
        "\n",
        "# Dataframe with 1 partition\n",
        "dataframe.coalesce(1).rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DucxsgFaz_-G"
      },
      "source": [
        "### Running SQL Queries Programmatically\n",
        "\n",
        "Raw SQL queries can also be used by enabling the “sql” operation on our SparkSession to run SQL queries programmatically and return the result sets as DataFrame structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5cAr1Oaz_-G"
      },
      "outputs": [],
      "source": [
        "# Registering a table\n",
        "dataframe.registerTempTable(\"df\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEwpuarPz_-G",
        "outputId": "c03fe097-0872-4a56-9e1b-58e71287340b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|                 _id|                 URL|         author| bestsellers_date|         description|        price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
            "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|  Dean R Koontz|{{1211587200000}}|Odd Thomas, who c...|   {null, 27}|{{1212883200000}}|       Bantam| {1}|           {0}|           ODD HOURS|          {1}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|{{1211587200000}}|Aliens have taken...|{25.99, null}|{{1212883200000}}|Little, Brown| {2}|           {1}|            THE HOST|          {3}|This is a new column|\n",
            "|{5b4aa4ead3089013...|http://www.amazon...|   Emily Giffin|{{1211587200000}}|A woman's happy m...|{24.95, null}|{{1212883200000}}| St. Martin's| {3}|           {2}|LOVE THE ONE YOU'...|          {2}|This is a new column|\n",
            "+--------------------+--------------------+---------------+-----------------+--------------------+-------------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+---------------+-----+\n",
            "|         Themes|count|\n",
            "+---------------+-----+\n",
            "|    Anger_Theme|  203|\n",
            "|   Other_Themes| 8778|\n",
            "|  Mystery_Theme|  454|\n",
            "|     Hate_Theme|   23|\n",
            "| Criminal_Theme|  305|\n",
            "|   Horror_Theme|    6|\n",
            "|Happiness_Theme|   34|\n",
            "|     Love_Theme|  392|\n",
            "+---------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sc.sql(\"select * from df\").show(3)\n",
        "\n",
        "\n",
        "sc.sql(\"select \\\n",
        "               CASE WHEN description LIKE '%love%' THEN 'Love_Theme' \\\n",
        "               WHEN description LIKE '%hate%' THEN 'Hate_Theme' \\\n",
        "               WHEN description LIKE '%happy%' THEN 'Happiness_Theme' \\\n",
        "               WHEN description LIKE '%anger%' THEN 'Anger_Theme' \\\n",
        "               WHEN description LIKE '%horror%' THEN 'Horror_Theme' \\\n",
        "               WHEN description LIKE '%death%' THEN 'Criminal_Theme' \\\n",
        "               WHEN description LIKE '%detective%' THEN 'Mystery_Theme' \\\n",
        "               ELSE 'Other_Themes' \\\n",
        "               END Themes \\\n",
        "       from df\").groupBy('Themes').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLS2aOMGz_-G"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiMuUTYJz_-G"
      },
      "outputs": [],
      "source": [
        "# Converting dataframe into an RDD\n",
        "rdd_convert = dataframe.rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbNxLihmz_-G",
        "outputId": "62d929a6-6a7c-42df-bf90-d4da426611fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"_id\":{\"$oid\":\"5b4aa4ead3089013507db18b\"},\"URL\":\"http://www.amazon.com/Odd-Hours-Dean-Koontz/dp/0553807056?tag=NYTBS-20\",\"author\":\"Dean R Koontz\",\"bestsellers_date\":{\"$date\":{\"$numberLong\":\"1211587200000\"}},\"description\":\"Odd Thomas, who can communicate with the dead, confronts evil forces in a California coastal town.\",\"price\":{\"$numberInt\":\"27\"},\"published_date\":{\"$date\":{\"$numberLong\":\"1212883200000\"}},\"publisher\":\"Bantam\",\"rank\":{\"$numberInt\":\"1\"},\"rank_last_week\":{\"$numberInt\":\"0\"},\"title\":\"ODD HOURS\",\"weeks_on_list\":{\"$numberInt\":\"1\"},\"new_column\":\"This is a new column\"}'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting dataframe into a RDD of string\n",
        "dataframe.toJSON().first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byvdgs8fz_-G",
        "outputId": "a59257c4-092e-4032-97e1-dd9e6302782a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
            "  Nested StructType not supported in conversion to Arrow\n",
            "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>URL</th>\n",
              "      <th>author</th>\n",
              "      <th>bestsellers_date</th>\n",
              "      <th>description</th>\n",
              "      <th>price</th>\n",
              "      <th>published_date</th>\n",
              "      <th>publisher</th>\n",
              "      <th>rank</th>\n",
              "      <th>rank_last_week</th>\n",
              "      <th>title</th>\n",
              "      <th>weeks_on_list</th>\n",
              "      <th>new_column</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(5b4aa4ead3089013507db18b,)</td>\n",
              "      <td>http://www.amazon.com/Odd-Hours-Dean-Koontz/dp...</td>\n",
              "      <td>Dean R Koontz</td>\n",
              "      <td>((1211587200000,),)</td>\n",
              "      <td>Odd Thomas, who can communicate with the dead,...</td>\n",
              "      <td>(None, 27)</td>\n",
              "      <td>((1212883200000,),)</td>\n",
              "      <td>Bantam</td>\n",
              "      <td>(1,)</td>\n",
              "      <td>(0,)</td>\n",
              "      <td>ODD HOURS</td>\n",
              "      <td>(1,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(5b4aa4ead3089013507db18c,)</td>\n",
              "      <td>http://www.amazon.com/The-Host-Novel-Stephenie...</td>\n",
              "      <td>Stephenie Meyer</td>\n",
              "      <td>((1211587200000,),)</td>\n",
              "      <td>Aliens have taken control of the minds and bod...</td>\n",
              "      <td>(25.99, None)</td>\n",
              "      <td>((1212883200000,),)</td>\n",
              "      <td>Little, Brown</td>\n",
              "      <td>(2,)</td>\n",
              "      <td>(1,)</td>\n",
              "      <td>THE HOST</td>\n",
              "      <td>(3,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(5b4aa4ead3089013507db18d,)</td>\n",
              "      <td>http://www.amazon.com/Love-Youre-With-Emily-Gi...</td>\n",
              "      <td>Emily Giffin</td>\n",
              "      <td>((1211587200000,),)</td>\n",
              "      <td>A woman's happy marriage is shaken when she en...</td>\n",
              "      <td>(24.95, None)</td>\n",
              "      <td>((1212883200000,),)</td>\n",
              "      <td>St. Martin's</td>\n",
              "      <td>(3,)</td>\n",
              "      <td>(2,)</td>\n",
              "      <td>LOVE THE ONE YOU'RE WITH</td>\n",
              "      <td>(2,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(5b4aa4ead3089013507db18e,)</td>\n",
              "      <td>http://www.amazon.com/The-Front-Garano-Patrici...</td>\n",
              "      <td>Patricia Cornwell</td>\n",
              "      <td>((1211587200000,),)</td>\n",
              "      <td>A Massachusetts state investigator and his tea...</td>\n",
              "      <td>(22.95, None)</td>\n",
              "      <td>((1212883200000,),)</td>\n",
              "      <td>Putnam</td>\n",
              "      <td>(4,)</td>\n",
              "      <td>(0,)</td>\n",
              "      <td>THE FRONT</td>\n",
              "      <td>(1,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(5b4aa4ead3089013507db18f,)</td>\n",
              "      <td>http://www.amazon.com/Snuff-Chuck-Palahniuk/dp...</td>\n",
              "      <td>Chuck Palahniuk</td>\n",
              "      <td>((1211587200000,),)</td>\n",
              "      <td>An aging porn queens aims to cap her career by...</td>\n",
              "      <td>(24.95, None)</td>\n",
              "      <td>((1212883200000,),)</td>\n",
              "      <td>Doubleday</td>\n",
              "      <td>(5,)</td>\n",
              "      <td>(0,)</td>\n",
              "      <td>SNUFF</td>\n",
              "      <td>(1,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10190</th>\n",
              "      <td>(5b4aa4ead3089013507dd959,)</td>\n",
              "      <td>https://www.amazon.com/Clancy-Line-Sight-Jack-...</td>\n",
              "      <td>Mike Maden</td>\n",
              "      <td>((1530921600000,),)</td>\n",
              "      <td>Jack Ryan Jr. risks his life to protect a woma...</td>\n",
              "      <td>(None, 0)</td>\n",
              "      <td>((1532217600000,),)</td>\n",
              "      <td>Putnam</td>\n",
              "      <td>(11,)</td>\n",
              "      <td>(6,)</td>\n",
              "      <td>TOM CLANCY LINE OF SIGHT</td>\n",
              "      <td>(4,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10191</th>\n",
              "      <td>(5b4aa4ead3089013507dd95a,)</td>\n",
              "      <td>https://www.amazon.com/Something-Water-Novel-C...</td>\n",
              "      <td>Catherine Steadman</td>\n",
              "      <td>((1530921600000,),)</td>\n",
              "      <td>A documentary filmmaker and an investment bank...</td>\n",
              "      <td>(None, 0)</td>\n",
              "      <td>((1532217600000,),)</td>\n",
              "      <td>Ballantine</td>\n",
              "      <td>(12,)</td>\n",
              "      <td>(11,)</td>\n",
              "      <td>SOMETHING IN THE WATER</td>\n",
              "      <td>(5,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10192</th>\n",
              "      <td>(5b4aa4ead3089013507dd95b,)</td>\n",
              "      <td>https://www.amazon.com/Little-Fires-Everywhere...</td>\n",
              "      <td>Celeste Ng</td>\n",
              "      <td>((1530921600000,),)</td>\n",
              "      <td>An artist upends a quiet town outside Cleveland.</td>\n",
              "      <td>(None, 0)</td>\n",
              "      <td>((1532217600000,),)</td>\n",
              "      <td>Penguin Press</td>\n",
              "      <td>(13,)</td>\n",
              "      <td>(12,)</td>\n",
              "      <td>LITTLE FIRES EVERYWHERE</td>\n",
              "      <td>(41,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10193</th>\n",
              "      <td>(5b4aa4ead3089013507dd95c,)</td>\n",
              "      <td>https://www.amazon.com/Shelter-Place-Nora-Robe...</td>\n",
              "      <td>Nora Roberts</td>\n",
              "      <td>((1530921600000,),)</td>\n",
              "      <td>Survivors of a mass shooting outside a mall in...</td>\n",
              "      <td>(None, 0)</td>\n",
              "      <td>((1532217600000,),)</td>\n",
              "      <td>St. Martin's</td>\n",
              "      <td>(14,)</td>\n",
              "      <td>(5,)</td>\n",
              "      <td>SHELTER IN PLACE</td>\n",
              "      <td>(6,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10194</th>\n",
              "      <td>(5b4aa4ead3089013507dd95d,)</td>\n",
              "      <td>https://www.amazon.com/Last-Time-Lied-Novel/dp...</td>\n",
              "      <td>Riley Sager</td>\n",
              "      <td>((1530921600000,),)</td>\n",
              "      <td>A painter is in danger when she returns to the...</td>\n",
              "      <td>(None, 0)</td>\n",
              "      <td>((1532217600000,),)</td>\n",
              "      <td>Dutton</td>\n",
              "      <td>(15,)</td>\n",
              "      <td>(0,)</td>\n",
              "      <td>THE LAST TIME I LIED</td>\n",
              "      <td>(1,)</td>\n",
              "      <td>This is a new column</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10195 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               _id  \\\n",
              "0      (5b4aa4ead3089013507db18b,)   \n",
              "1      (5b4aa4ead3089013507db18c,)   \n",
              "2      (5b4aa4ead3089013507db18d,)   \n",
              "3      (5b4aa4ead3089013507db18e,)   \n",
              "4      (5b4aa4ead3089013507db18f,)   \n",
              "...                            ...   \n",
              "10190  (5b4aa4ead3089013507dd959,)   \n",
              "10191  (5b4aa4ead3089013507dd95a,)   \n",
              "10192  (5b4aa4ead3089013507dd95b,)   \n",
              "10193  (5b4aa4ead3089013507dd95c,)   \n",
              "10194  (5b4aa4ead3089013507dd95d,)   \n",
              "\n",
              "                                                     URL              author  \\\n",
              "0      http://www.amazon.com/Odd-Hours-Dean-Koontz/dp...       Dean R Koontz   \n",
              "1      http://www.amazon.com/The-Host-Novel-Stephenie...     Stephenie Meyer   \n",
              "2      http://www.amazon.com/Love-Youre-With-Emily-Gi...        Emily Giffin   \n",
              "3      http://www.amazon.com/The-Front-Garano-Patrici...   Patricia Cornwell   \n",
              "4      http://www.amazon.com/Snuff-Chuck-Palahniuk/dp...     Chuck Palahniuk   \n",
              "...                                                  ...                 ...   \n",
              "10190  https://www.amazon.com/Clancy-Line-Sight-Jack-...          Mike Maden   \n",
              "10191  https://www.amazon.com/Something-Water-Novel-C...  Catherine Steadman   \n",
              "10192  https://www.amazon.com/Little-Fires-Everywhere...          Celeste Ng   \n",
              "10193  https://www.amazon.com/Shelter-Place-Nora-Robe...        Nora Roberts   \n",
              "10194  https://www.amazon.com/Last-Time-Lied-Novel/dp...         Riley Sager   \n",
              "\n",
              "          bestsellers_date                                        description  \\\n",
              "0      ((1211587200000,),)  Odd Thomas, who can communicate with the dead,...   \n",
              "1      ((1211587200000,),)  Aliens have taken control of the minds and bod...   \n",
              "2      ((1211587200000,),)  A woman's happy marriage is shaken when she en...   \n",
              "3      ((1211587200000,),)  A Massachusetts state investigator and his tea...   \n",
              "4      ((1211587200000,),)  An aging porn queens aims to cap her career by...   \n",
              "...                    ...                                                ...   \n",
              "10190  ((1530921600000,),)  Jack Ryan Jr. risks his life to protect a woma...   \n",
              "10191  ((1530921600000,),)  A documentary filmmaker and an investment bank...   \n",
              "10192  ((1530921600000,),)   An artist upends a quiet town outside Cleveland.   \n",
              "10193  ((1530921600000,),)  Survivors of a mass shooting outside a mall in...   \n",
              "10194  ((1530921600000,),)  A painter is in danger when she returns to the...   \n",
              "\n",
              "               price       published_date      publisher   rank  \\\n",
              "0         (None, 27)  ((1212883200000,),)         Bantam   (1,)   \n",
              "1      (25.99, None)  ((1212883200000,),)  Little, Brown   (2,)   \n",
              "2      (24.95, None)  ((1212883200000,),)   St. Martin's   (3,)   \n",
              "3      (22.95, None)  ((1212883200000,),)         Putnam   (4,)   \n",
              "4      (24.95, None)  ((1212883200000,),)      Doubleday   (5,)   \n",
              "...              ...                  ...            ...    ...   \n",
              "10190      (None, 0)  ((1532217600000,),)         Putnam  (11,)   \n",
              "10191      (None, 0)  ((1532217600000,),)     Ballantine  (12,)   \n",
              "10192      (None, 0)  ((1532217600000,),)  Penguin Press  (13,)   \n",
              "10193      (None, 0)  ((1532217600000,),)   St. Martin's  (14,)   \n",
              "10194      (None, 0)  ((1532217600000,),)         Dutton  (15,)   \n",
              "\n",
              "      rank_last_week                     title weeks_on_list  \\\n",
              "0               (0,)                 ODD HOURS          (1,)   \n",
              "1               (1,)                  THE HOST          (3,)   \n",
              "2               (2,)  LOVE THE ONE YOU'RE WITH          (2,)   \n",
              "3               (0,)                 THE FRONT          (1,)   \n",
              "4               (0,)                     SNUFF          (1,)   \n",
              "...              ...                       ...           ...   \n",
              "10190           (6,)  TOM CLANCY LINE OF SIGHT          (4,)   \n",
              "10191          (11,)    SOMETHING IN THE WATER          (5,)   \n",
              "10192          (12,)   LITTLE FIRES EVERYWHERE         (41,)   \n",
              "10193           (5,)          SHELTER IN PLACE          (6,)   \n",
              "10194           (0,)      THE LAST TIME I LIED          (1,)   \n",
              "\n",
              "                 new_column  \n",
              "0      This is a new column  \n",
              "1      This is a new column  \n",
              "2      This is a new column  \n",
              "3      This is a new column  \n",
              "4      This is a new column  \n",
              "...                     ...  \n",
              "10190  This is a new column  \n",
              "10191  This is a new column  \n",
              "10192  This is a new column  \n",
              "10193  This is a new column  \n",
              "10194  This is a new column  \n",
              "\n",
              "[10195 rows x 13 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Obtaining contents of df as Pandas dataFrame\n",
        "dataframe.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGUqs-Mzz_-G"
      },
      "outputs": [],
      "source": [
        "# Write & Save File in .parquet format\n",
        "dataframe.select(\"author\", \"title\", \"rank\", \"description\")\\\n",
        ".write \\\n",
        ".save(\"Rankings_Descriptions.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCsXTXhgz_-G"
      },
      "outputs": [],
      "source": [
        "# Write & Save File in .json format\n",
        "dataframe.select(\"author\", \"title\") \\\n",
        ".write \\\n",
        ".save(\"Authors_Titles.json\",format=\"json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuPW1W9sz_-G"
      },
      "outputs": [],
      "source": [
        "# End Spark Session \n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nNWNqLYz_-G"
      },
      "source": [
        "## Multiclass Text Classifications\n",
        "\n",
        "Loading a CSV file dataset [here](https://www.kaggle.com/kaggle/san-francisco-crime-classification?select=train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnLz3NYEz_-G"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "\n",
        "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbQajuHNz_-G"
      },
      "source": [
        "Remove the columns we do not need and have a look the first five rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBdp_08Xz_-G",
        "outputId": "173b689a-49d9-467e-9cb1-8772afc8b47f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+\n",
            "|      Category|            Descript|\n",
            "+--------------+--------------------+\n",
            "|      WARRANTS|      WARRANT ARREST|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
            "+--------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
        "data = data.select([column for column in data.columns if column not in drop_list])\n",
        "data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfs6q8hmz_-G"
      },
      "source": [
        "Apply printSchema() on the data which will print the schema in a tree format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD2gX1t8z_-G",
        "outputId": "7b117244-f4a0-400a-f4fc-ca156358e89e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Descript: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv8FMkXqz_-H"
      },
      "source": [
        "Top 20 crime categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qKLkfaXz_-H",
        "outputId": "d442dc53-771e-46e3-bee1-e34e3f41e473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|            Category| count|\n",
            "+--------------------+------+\n",
            "|       LARCENY/THEFT|174900|\n",
            "|      OTHER OFFENSES|126182|\n",
            "|        NON-CRIMINAL| 92304|\n",
            "|             ASSAULT| 76876|\n",
            "|       DRUG/NARCOTIC| 53971|\n",
            "|       VEHICLE THEFT| 53781|\n",
            "|           VANDALISM| 44725|\n",
            "|            WARRANTS| 42214|\n",
            "|            BURGLARY| 36755|\n",
            "|      SUSPICIOUS OCC| 31414|\n",
            "|      MISSING PERSON| 25989|\n",
            "|             ROBBERY| 23000|\n",
            "|               FRAUD| 16679|\n",
            "|FORGERY/COUNTERFE...| 10609|\n",
            "|     SECONDARY CODES|  9985|\n",
            "|         WEAPON LAWS|  8555|\n",
            "|        PROSTITUTION|  7484|\n",
            "|            TRESPASS|  7326|\n",
            "|     STOLEN PROPERTY|  4540|\n",
            "|SEX OFFENSES FORC...|  4388|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "data.groupBy(\"Category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihtp10iNz_-H"
      },
      "source": [
        "Top 20 crime descriptions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARa6EruCz_-H",
        "outputId": "3189ae04-df2b-414c-935b-f2e506c7f8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            Descript|count|\n",
            "+--------------------+-----+\n",
            "|GRAND THEFT FROM ...|60022|\n",
            "|       LOST PROPERTY|31729|\n",
            "|             BATTERY|27441|\n",
            "|   STOLEN AUTOMOBILE|26897|\n",
            "|DRIVERS LICENSE, ...|26839|\n",
            "|      WARRANT ARREST|23754|\n",
            "|SUSPICIOUS OCCURR...|21891|\n",
            "|AIDED CASE, MENTA...|21497|\n",
            "|PETTY THEFT FROM ...|19771|\n",
            "|MALICIOUS MISCHIE...|17789|\n",
            "|   TRAFFIC VIOLATION|16471|\n",
            "|PETTY THEFT OF PR...|16196|\n",
            "|MALICIOUS MISCHIE...|15957|\n",
            "|THREATS AGAINST LIFE|14716|\n",
            "|      FOUND PROPERTY|12146|\n",
            "|ENROUTE TO OUTSID...|11470|\n",
            "|GRAND THEFT OF PR...|11010|\n",
            "|POSSESSION OF NAR...|10050|\n",
            "|PETTY THEFT FROM ...|10029|\n",
            "|PETTY THEFT SHOPL...| 9571|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.groupBy(\"Descript\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYp93N4Oz_-H"
      },
      "source": [
        "Model Pipeline\n",
        "\n",
        "Spark Machine Learning Pipelines API is similar to Scikit-Learn. Spark pipeline includes three steps:\n",
        "\n",
        "- regexTokenizer: Tokenization (with Regular Expression)\n",
        "- stopwordsRemover: Remove Stop Words\n",
        "- countVectors: Count vectors (“document-term vectors”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXa8wtsjz_-H"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# regular expression tokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# stop words\n",
        "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "\n",
        "# bag of words count\n",
        "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GbwFuX1z_-H"
      },
      "source": [
        "StringIndexer\n",
        "\n",
        "StringIndexer encodes a string column of labels to a column of label indices. The indices are in `(0, numLabels)`, ordered by label frequencies, so the most frequent label gets index `0`.\n",
        "\n",
        "In our case, the label column (Category) will be encoded to label indices, from 0 to 32; the most frequent label (LARCENY/THEFT) will be indexed as 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ljHZEAURz_-H",
        "outputId": "932cc37b-2e30-4536-e3b8-92f20581b198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|      Category|            Descript|               words|            filtered|            features|label|\n",
            "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(809,[17,32],[1.0...|  7.0|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
            "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
            "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
            "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "dataset.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YArKmhZ1z_-H"
      },
      "source": [
        "Partition Training & Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bXCMJ7Mz_-H",
        "outputId": "ecfc8e41-7d9f-495a-b821-b29eb5ce2b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 614691\n",
            "Test Dataset Count: 263358\n"
          ]
        }
      ],
      "source": [
        "# set seed for reproducibility\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H10gvprz_-H"
      },
      "source": [
        "Model Training and Evaluation\n",
        "\n",
        "Logistic Regression using Count Vector Features\n",
        "\n",
        "Our model will make predictions and score on the test set; we then look at the top 10 predictions from the highest probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMrkFSY5z_-H",
        "outputId": "5ac2b7ae-fb10-4d9a-b737-2a9f14fe250a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|                      Descript|     Category|                   probability|label|prediction|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8730172912256745,0.02054...|  0.0|       0.0|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpbD9pHzz_-H",
        "outputId": "a1ec9e01-b63b-4de6-f568-137d2cc5e23b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9721281687008145"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzeaT_mdz_-H"
      },
      "source": [
        "Logistic Regression using TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_5Pl2ubz_-H",
        "outputId": "22ff7973-1227-42a8-d7fe-58e43d6d33b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|                      Descript|     Category|                   probability|label|prediction|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8846868443884813,0.01886...|  0.0|       0.0|\n",
            "+------------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHJQ-76Jz_-H",
        "outputId": "006f2320-7d21-4929-821a-c994bf417770"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9721507088140049"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf8BsIDaz_-H"
      },
      "source": [
        "Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4oo5dMMz_-H",
        "outputId": "a1b40d43-086b-4eb8-9514-301be3d22aff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9918960725584134"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Create ParamGrid for Cross Validation\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
        "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
        "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
        "             .build())\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=lr, \\\n",
        "                    estimatorParamMaps=paramGrid, \\\n",
        "                    evaluator=evaluator, \\\n",
        "                    numFolds=5)\n",
        "cvModel = cv.fit(trainingData)\n",
        "\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Evaluate best model\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8BvHaXWz_-H"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91ObCJIyz_-I",
        "outputId": "b20d9cf1-dbae-454b-f325-9d811dde0715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "|                    Descript|     Category|                   probability|label|prediction|\n",
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.999999999986501,1.521776...|  0.0|       0.0|\n",
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "\n",
        "nb = NaiveBayes(smoothing=1)\n",
        "model = nb.fit(trainingData)\n",
        "\n",
        "predictions = model.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ8SERMzz_-I",
        "outputId": "0974d552-3f50-427d-ac32-081c5236e6f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9934900857765636"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS24LUzOz_-I"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUX81FSJz_-I",
        "outputId": "27f2a712-3a69-4f36-fc5f-01e6564f5420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "|                    Descript|     Category|                   probability|label|prediction|\n",
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.564748070756399,0.073737...|  0.0|       0.0|\n",
            "+----------------------------+-------------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
        "                            featuresCol=\"features\", \\\n",
        "                            numTrees = 100, \\\n",
        "                            maxDepth = 4, \\\n",
        "                            maxBins = 32)\n",
        "\n",
        "# Train model with Training Data\n",
        "rfModel = rf.fit(trainingData)\n",
        "\n",
        "predictions = rfModel.transform(testData)\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6UFgv3Sz_-I",
        "outputId": "ed5dbec0-c529-447c-df09-3293d8da6d7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7153767272417834"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "60716cfbf8f6257f052921a35c79929cd52e6aa52b65b24e32100e25d3fdc1e3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}